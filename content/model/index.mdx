# isA Model Service

Unified AI model platform for inference, training, and generic ML prediction.

## Installation

```bash
pip install isa-model
```

## Quick Start

```python
from isa_model import AsyncISAModel

async with AsyncISAModel(base_url="http://localhost:8082") as client:
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    print(response.choices[0].message.content)
```

## Latest Updates (2026-02-14)

- **Generic ML prediction API added** with dedicated training and inference routes.
- **Prediction endpoint pagination added** (`limit` / `offset`) for model and prediction lists.
- **Tenant isolation enforced** for prediction model storage and retrieval.
- **Prediction reliability/security hardening**:
  - training data key validation (`ds`, `y`)
  - frequency / period bounds
  - sanitized error persistence
  - deleted-model safety checks during background training
  - HMAC integrity checks for saved model artifacts
  - tenant-scoped LRU model caching and eviction limits
- **Auth integration improved** by validating API keys through the auth service.
- **Optional dependency guards** added for Lightning algorithm modules.

## Core Capabilities

### Inference

- OpenAI-compatible chat/completions interface
- Multi-provider routing (OpenAI, Anthropic, Ollama, Cerebras, YYDS, Replicate)
- Streaming, tool-calling, and structured outputs
- Multi-modal support (text, vision, audio, embedding, image, video)

### Caching

- L1 exact-match cache (Redis)
- L2 semantic cache (Qdrant)
- Cache stats and invalidation endpoints

### Training

- Lightning training pipeline endpoints for trace-based optimization
- Algorithm support for `closed_loop`, `apo`, `grpo`, and `custom`

### Generic ML Prediction

- Model lifecycle for prediction models (create/train/list/get/delete)
- Inference endpoint for forecasts
- Tenant-scoped prediction history with pagination

## API Endpoints

Base URL: `http://localhost:8082`

### Inference & Cache

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/invoke` | POST | Unified inference endpoint |
| `/api/v1/invoke/v2` | POST | Extended invoke route |
| `/api/v1/models` | GET | List available LLM models |
| `/api/v1/cache/stats` | GET | Cache statistics |
| `/api/v1/cache/invalidate/{provider}/{model}` | POST | Invalidate cache by provider/model |
| `/api/v1/cache/clear` | POST | Clear all cache entries |

### Training

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/models/training/train` | POST | Start Lightning training |
| `/api/v1/models/training/stats` | GET | Training statistics |
| `/api/v1/models/training/export` | POST | Export training data |
| `/api/v1/models/training/config` | GET | Read active training config |

### Prediction (ML)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/models/training/ml/prediction/models` | POST | Create + start training prediction model |
| `/api/v1/models/training/ml/prediction/models` | GET | List prediction models (`limit`, `offset`) |
| `/api/v1/models/training/ml/prediction/models/{model_id}` | GET | Get prediction model details |
| `/api/v1/models/training/ml/prediction/models/{model_id}` | DELETE | Delete prediction model |
| `/api/v1/models/inference/ml/prediction/models/{model_id}/predict` | POST | Run prediction |
| `/api/v1/models/inference/ml/prediction/models/{model_id}/predictions` | GET | List prediction runs (`limit`, `offset`) |

## Prediction Example

```bash
# 1) Create + train a model (prophet)
curl -X POST http://localhost:8082/api/v1/models/training/ml/prediction/models \
  -H "Content-Type: application/json" \
  -d '{
    "name": "daily-sales-forecast",
    "model_type": "prophet",
    "data": [
      {"ds": "2026-01-01", "y": 120},
      {"ds": "2026-01-02", "y": 125}
    ]
  }'

# 2) Run prediction
curl -X POST http://localhost:8082/api/v1/models/inference/ml/prediction/models/<MODEL_ID>/predict \
  -H "Content-Type: application/json" \
  -d '{
    "periods": 14,
    "frequency": "D",
    "include_history": false
  }'
```

## Documentation

### Getting Started
- [Quick Start](./quickstart) - First steps
- [Providers](./providers) - Provider configuration

### Features
- [LLM Services](./llm-services) - Text generation and routing
- [Tool Calling](./tool-calling) - Function calling
- [Caching](./caching) - Cache architecture and tuning

## Environment Variables

| Variable | Description |
|----------|-------------|
| `OPENAI_API_KEY` | OpenAI provider key |
| `ANTHROPIC_API_KEY` | Anthropic provider key |
| `YYDS_API_KEY` | YYDS provider key |
| `REDIS_URL` | Redis URL for caching |
| `QDRANT_URL` | Qdrant URL for semantic cache |
| `NATS_URL` | NATS URL for events |

